rule cdhit_faa_single:
    """Clustering predicted genes using cdhit"""
    message:
        "CD-HIT clustering {input} -> {output}"
    input:
        "{dir}/{file}.faa"
    output:
        fa="{dir}.otu/{file}.NR.faa",
        clstr="{dir}.otu/{file}.NR.faa.clstr"
    log:
        "{dir}.otu/{file}.NR.faa.log"
    threads:
        33
    params:
        slow=1,
        print_overlap=1,
        description_length=0
    shell: """
    cd-hit \
    -T {threads} \
    -M $[1024 * 4 * {threads}] \
    -i {input} \
    -o {output.fa} \
    -g {params.slow} \
    -p {params.print_overlap} \
    -d {params.description_length} \
    > {log} 2>&1
    """

rule cdhit_clstr_to_csv:
    input:
        "{dir}/{file}.NR.faa.clstr"
    output:
        "{dir}/{file}.NR.csv"
    threads:
        1
    run:
        import re, csv
        clstr_format = re.compile(
            "(?P<leaf_id>\d+)\s+(?P<qlen>\d+)aa,\s"
            ">(?P<qacc>.*)\.\.\.\s"
            "(at\s(?P<qstart>\d+):(?P<qend>\d+):(?P<sstart>\d+):(?P<send>\d+)/"
            "(?P<pident>\d+\.\d+)%|\*)"
            , flags = re.VERBOSE|re.ASCII)
        fieldnames = ["cluster_id", "leaf_id",
                      "sacc", "qacc", "qlen",
                      "qstart", "qend", "sstart", "send",
                      "pident"]
        with open(input[0], "r") as inf, \
             open(output[0], "w") as outf:
            writer = csv.DictWriter(outf, fieldnames=fieldnames)
            writer.writeheader()
            rows=[]
            cluster_id = None
            sacc = None
            for line in inf:
                if line[0] == ">":
                    if len(rows) > 0:
                        for row in rows:
                            row['sacc'] = sacc
                        writer.writerows(rows)
                        rows=[]
                    cluster_id = line.split()[1].strip()
                else:
                    d = clstr_format.match(line).groupdict()
                    d["cluster_id"] = cluster_id
                    if "qstart" not in d or d["qstart"] is None:
                        d["qstart"] = 1
                        d["qend"] = d["qlen"]
                        d["sstart"] = 1
                        d["send"] = d["qlen"]
                        d["pident"] = "100.00"
                        sacc = d["qacc"]
                    rows.append(d)

            if len(rows) > 0:
                for row in rows:
                    row['sacc'] = sacc
                writer.writerows(rows)


rule make_otu_table:
    output:
        table="{dir}.otu/{query}.{gene}.otu_table.csv",
        counts="{dir}.otu/{query}.{gene}.otu_count.csv"
    input:
        clust="{dir}.otu/{query}.{gene}.NR.csv",
        cov="{dir}.cov/{query}.cov"
    run:
        import csv
        from collections import defaultdict
        map_seq_to_clust = {}
        clusters = set()
        samples = set()
        # cluster_id,leaf_id,sacc,qacc,qlen,qstart,qend,sstart,send,pident
        # 175,6,4021.contigs.k119_34902.51644.52807,4023.contigs.k119_1016.316.1059,248,1,248,1,248,100.00
        with open(input.clust, "r") as cluster_f:
            cluster_reader = csv.DictReader(cluster_f)
            for row in cluster_reader:
                map_seq_to_clust[row['qacc']] = row['sacc']
                clusters.add(row['sacc'])

        coverages = { centroid:defaultdict(float) for centroid in clusters }
        coverage_counts = { centroid:defaultdict(float) for centroid in clusters }

        # target,source,sacc,start,end,avg,max,med,min,q23,std,sum
        # 4023,4023,k119_13047,623,180,5.70203160271,12.0,5.0,1.0,5.31390134529,2.60866249754,2526.0
        lost = 0
        found = 0
        with open(input.cov, "r") as coverage_f:
            coverage_reader = csv.DictReader(coverage_f)
            for row in coverage_reader:
                seq_name = "{target}.contigs.{sacc}.{start}.{end}".format(**row)
                if seq_name in map_seq_to_clust:
                    samples.add(row['source'])
                    centroid = map_seq_to_clust[seq_name]
                    coverages[centroid][row['source']]+=float(row['q23'])
                    coverage_counts[centroid][row['source']]+=1
                    found += 1
                else:
                    lost +=1

        with open(output.table, "w") as out_f:
            table_writer = csv.DictWriter(out_f, fieldnames = ["centroid"]+list(samples))
            table_writer.writeheader()
            for centroid in coverages:
                coverages[centroid]['centroid']=centroid
                table_writer.writerow(coverages[centroid])

        with open(output.counts, "w") as out_f:
            table_writer = csv.DictWriter(out_f, fieldnames = ["centroid"]+list(samples))
            table_writer.writeheader()
            for centroid in coverages:
                coverage_counts[centroid]['centroid']=centroid
                table_writer.writerow(coverage_counts[centroid])


rule cdhit_fna_single:
    """Clustering predicted genes (nuc) using cdhit-est"""
    message:
        "CD-HIT-EST clustering {input} -> {output}"
    input:
        "{dir}.genes/{nodots}.fna"
    output:
        "{dir}.genes/{nodots}.NR.fna"
    log:
        "{dir}.genes/{nodots}.NR.fna.log"
    threads:
        33
    params:
        slow=1,
        print_overlap=1,
        description_length=0,
        id=0.95
    shell: """
    cd-hit-est \
    -i {input} \
    -o {output} \
    -c {params.id} \
    -M $[{threads} * 1024 * 4] \
    -T {threads} \
    -g {params.slow} \
    -p {params.print_overlap} \
    -d {params.description_length} \
    > {log} 2>&1
    """
