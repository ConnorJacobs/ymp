from ymp.config import icfg
import ymp
import os


"""
This file contains the rules dealing with FQ file provisioning and preparation. 
"""

###
### Meta rules
###

localrules: fq_all
rule fq_all:
    message: "Finished {wildcards.dir}"
    input: "{dir}/{:fq_names:}.fq.gz"
    output: touch("{dir}/all")

rule fastqc_all:
    message: "Finished {wildcards.dir}"
    input: "{dir}.fastqc/{:fq_names:}_fastqc.zip"
    output: touch("{dir}.fastqc/all")



###
###  SRA access 
###


localrules: prefetch
rule prefetch:
    """
    Downloads SRA files into NCBI SRA folder (ncbi/public/sra).
    """
    message:
        "Pre-Fetching {wildcards.SRR}"
    output:
        "{:dir.sra:}/{SRR}.sra"
    conda:
        "sratools.yml"
    shell: """
    prefetch {wildcards.SRR}
    """


rule fastq_dump:
    """
    Extracts FQ from SRA files
    """
    message:
        "Extracting FastQ from {wildcards.SRR}"
    output:
        "{:dir.scratch:}/SRR/{SRR}_1.fastq.gz",
        "{:dir.scratch:}/SRR/{SRR}_2.fastq.gz"
    params:
        outdir = icfg.dir.scratch + "/SRR",
        p = lambda wc,threads: int(threads/2+.5)
    conda:
        "sratools.yml"
    threads:
        4
    # FIXME
    # the two cut processes use about 1 cpu each, fastqdump 1/4 and pgzip about 1 each.
    # not ideal. not sure why cut needs so much time. 
    shell: """
    fastq-dump {wildcards.SRR} \
        --split-files \
        --readids \
        --dumpbase \
        --skip-technical \
        --clip \
        --read-filter pass \
        --stdout | \
      paste - - - -  - - - - | \
      tee >(cut -f 1-4 | tr "\t" "\\n" | pigz -p {params.p} > {output[0]}) | \
      cut -f 5-8 | tr "\t" "\\n" | pigz -p {params.p} > {output[1]}
    """


rule export_qiime_map_file:
    message:
        "Creating Qiime map file for project {wildcards.project}"
    output:
        "{project}/qiime_mapping.tsv"
    run:
        import pandas as pd
        df = icfg[wildcards.project].run_data
        cols = df.columns.tolist()

        try:
            desc_idx = cols.index("Description")
            cols = cols[:desc_idx] + cols[desc_idx+1:] + [cols[desc_idx]]
            df = df[cols]
        except ValueError:
            df["Description"] = ""

        df.to_csv(output[0], sep="\t", index=False)

        # TODO: Rename bccol to "BarcodeSequence"
        #       Fake LinkerPrimerSequence col if not exists
        #       Make first/index column be called "#SampleID"


rule split_library:
    """
    Splits barcode indexed files into separate fq files
    """
    message:
        "Splitting library (barcodes: {input[0]}, reads: {input[1]})"
    input:
        lambda wc: icfg[wc.project].unsplit_path(wc.barcodes, wc.pairname),
        mapping = "{project}/qiime_mapping.tsv",
        tmpdir  = ancient(icfg.dir.tmp)
    output:
        outdir  = temp("{project}.split_libraries/{barcodes}/{pairname}/")
    log:
        "{project}.split_libraries/{barcodes}/{pairname}/split_library_log.txt",
        "{project}.split_libraries/{barcodes}/{pairname}/split_seq_file_log.txt",
    conda:
        "qiime.yml"
    shell: """
    split_libraries_fastq.py \
       -b {input[0]} \
       -i {input[1]} \
       -m {input.mapping} \
       --store_demultiplexed_fastq \
       --max_bad_run_length=1000000 \
       --min_per_read_length_fraction=0.000001 \
       --sequence_max_n=100000 \
       --phred_quality_threshold=1 \
       -o {output}

    split_sequence_file_on_sample_ids.py \
       -i {output}/seqs.fastq \
       --file_type fastq \
       -o {output} \
       > {log[1]} 2>&1
    """

rule split_library_compress_sample:
    message:
        "Compressing {wildcards.sample}.{wildcards.pairname}.fq.gz"
    input:
        "{project}.split_libraries/{barcodes}/{pairname}/"
    output:
        "{project}.split_libraries/{barcodes}/{sample}.{pairname}.fq.gz"
    conda:
        "pigz.yml"
    shell: """
    pigz -6 -c <{input}/{wildcards.sample}.fastq >{output}
    """

###
###  Linking into current workspace
###
    

localrules: symlink_raw_reads
rule symlink_raw_reads:
    """Normalize FQ names by creating symlinks to original files"""
    message:
        "Creating symlink {output} -> {input}"
    input:
        # extract path from config file:
        lambda wc: icfg[wc.project].FQpath(wc.run, wc.pairsuff)
    output:
        "{project}/{run}.{pairsuff}.fq.gz"
    run:
        if not os.path.isabs(input[0]):
            input[0] = os.path.join("..", input[0])
        os.symlink(input[0], output[0])



###
###  Quality Reports
###


rule fastqc:
    """Run FastQC on read files"""
    message: "Creating QC report for {input}"
    input:   "{dir}/{file}.fq.gz"
    output:  "{dir}.fastqc/{file}_fastqc.html",
             "{dir}.fastqc/{file}_fastqc.zip"
    log:     "{dir}.fastqc/{file}_fastqc.log"
    threads: 1
    params:  k=10,
             mem=icfg.mem("8g")
    conda:   "fastqc.yml"
    shell: """
    # override too low memory requested by fastqc 0.11.5 of 256M per thread:
    export _JAVA_OPTIONS="-Xmx{params.mem}m"
    fastqc -t {threads} -o $(dirname {output[0]}) {input} -k 10 >{log} 2>&1
    """


rule multiqc:
    """Assemble report on all FQ files in a directory"""
    message: "Aggregating QC reports for {wildcards.dir}"
    input:   "{dir}.fastqc/{:fq_names:}_fastqc.zip"
    output:  "{:dir.reports:}/{dir}.fastqc.html",
    log:     "{dir}.fastqc/multiqc.log"
    threads: 1
    params:  dir = "{dir}.fastqc"
    conda:   "multiqc.yml"
    shell: """
    multiqc \
            --module fastqc \
            --outdir {params.dir} \
            --title  {wildcards.dir} \
            --force \
            {wildcards.dir}.fastqc \
            > {log} 2>&1
    if [ -e  {params.dir}/multiqc_report.html ]; then
        mv {params.dir}/multiqc_report.html {output[0]}
    else
        # never versions of multiqc put title in output file name
        mv {params.dir}/{wildcards.dir}_multiqc_report.html {output[0]}
    fi
    """




###
###  Trimming
###


rule trimmomatic_adapter:
    """Trimming with Trimmomatic"""
    message:
        "Trimmomatic {wildcards.dir}/{wildcards.sample}"
    input:
        "{dir}/{sample}.{:pairnames:}.fq.gz"
    wildcard_constraints:
        adapter="(N|T(2|3|32))"
    output:
        "{dir}.trimmomatic{adapter}/{sample}.{:pairnames[0]:}.fq.gz",
        "{dir}.trimmomatic{adapter}/{sample}.unpaired.{:pairnames[0]:}.fq.gz",
        "{dir}.trimmomatic{adapter}/{sample}.{:pairnames[1]:}.fq.gz",
        "{dir}.trimmomatic{adapter}/{sample}.unpaired.{:pairnames[1]:}.fq.gz"
    log:
        "{dir}.trimmomatic{adapter}/{sample}.log"
    params:
        seed_mismatches = 2,
        palindrome_clip_thresh = 30,
        simple_clip_thresh = 10,
        min_adapter_len = 8,
        keep_both_reads = "true",
        arg_pe="PE"
    conda:
        "trimmomatic.yml"
    threads:
        1
    shell:"""
    case {wildcards.adapter} in
      N)   ADAPTER=NexteraPE-PE.fa ;;
      T2)  ADAPTER=TruSeq2-PE.fa ;;
      T3)  ADAPTER=TruSeq3-PE.fa ;;
      T32) ADAPTER=TruSeq3-PE-2.fa ;;
    esac

    ADAPTER_DIR="$(dirname $(which trimmomatic))/../share/trimmomatic/adapters"

    CLIPARG="ILLUMINACLIP:$ADAPTER_DIR/$ADAPTER"
    CLIPARG="$CLIPARG:{params.seed_mismatches}"
    CLIPARG="$CLIPARG:{params.palindrome_clip_thresh}"
    CLIPARG="$CLIPARG:{params.simple_clip_thresh}"
    CLIPARG="$CLIPARG:{params.min_adapter_len}"
    CLIPARG="$CLIPARG:{params.keep_both_reads}"

    bash -x `which trimmomatic` {params.arg_pe} \
        -threads {threads} \
        -phred33 \
        {input} {output} \
        $CLIPARG >>{log} 2>&1
    """


rule trimmomatic_adapter_se:  # ymp: extends trimmomatic_adapter
    input:  "{dir}/{sample}.{:pairnames[0]:}.fq.gz"
    output: "{dir}.trimmomatic{adapter}/{sample}.{:pairnames[0]:}.fq.gz"
    params: arg_pe  = "SE",
            outargs = "{output}"


rule sickle:
    message:
        "Trimming with Sickle {wildcards.dir}/{wildcards.sample} "
        "(Q={params.qual} L={params.length})"
    input:
        "{dir}/{sample}.{:pairnames:}.fq.gz"
    output:
        "{dir}.sickle{Q}{L}/{sample}.{:pairnames:}.fq.gz",
        "{dir}.sickle{Q}{L}/{sample}.unpaired.fq.gz",
    log:
        "{dir}.sickle{Q}{L}/{sample}.log"
    wildcard_constraints:
        Q = "(Q\d+|)",
        L = "(L\d+|)",
    params:
        length=lambda wc: wc.L[1:] if wc.L else 20,
        qual=lambda wc: wc.Q[1:] if wc.Q else 20,
        arg_pe="pe",
        inout2="-r {input[1]} -p {output[1]} -s {output[2]}"
    threads: 1
    conda:
        "sickle.yml"
    shell:"""
    sickle {params.arg_pe} \
        -f {input[0]} \
        -o {output[0]} \
        {params.inout2} \
        --qual-type=sanger \
        --length-threshold={params.length} \
        --qual-threshold={params.qual} \
        --gzip-output \
        --no-fiveprime \
        > {log} 2>&1
    """


rule sickle_se:  # ymp: extends sickle
    input:  "{dir}/{sample}.{:pairnames[0]:}.fq.gz"
    output: "{dir}.sickle{Q}{L}/{sample}.{:pairnames[0]:}.fq.gz"
    params: arg_pe = "se",
            inout2 = ""


###
###  Contaminant filtering
###


bbfiles="scafstats refstats "
bbfiles+="covstats rpkm covhist basecov bincov"

bbstats =  "bhist qhist aqhist bqhist lhist ihist ehist qahist "
bbstats += "indelhist mhist gchist idhist statsfile"
bbstats = bbstats.split()

bbduk_stats = "bhist qhist qchist aqhist bqhist lhist gchist".split()

rule bbmap_makedb:
    message: "BBMap: preparing index for ref={input}"
    input:   "{path}/{file}.fasta.gz"
    output:  "{path}/{file}.bbidx/ref/genome/1/summary.txt",
             "{path}/{file}.bbidx"
    log:     "{path}/{file}.bbidx/bbmap_index.log"
    params:  path="{path}/{file}.bbidx/",
             mem=icfg.mem("80g")
    threads: 8
    conda:   "bbmap.yml"
    shell: """
    bbmap.sh \
        path={params.path} \
        ref={input} \
        threads={threads} \
        pigz unpigz \
        -Xmx{params.mem}m \
        >{log} 2>&1
    """


rule bbmap_split:
    message: "BBMap filtering by {wildcards.reference} - {wildcards.dir}/{wildcards.sample}"
    input:   "{dir}/{sample}.{:pairnames:}.fq.gz",
             reference = "{:dir.references:}/{reference}.bbidx"
    output:  clean="{dir}.bbmRM{reference}/{sample}.{:pairnames:}.fq.gz",
             human="{dir}.bbmKP{reference}/{sample}.{:pairnames:}.fq.gz",
             stats=expand("{{dir}}.bbmRM{{reference}}/{{sample}}.{x}",x=bbstats)
    log:     "{dir}.bbmRM{reference}/{sample}.log"
    params:  stats=lambda wc: expand("{x}={dir}.bbmRM{reference}/{sample}.{x}",x=bbstats,**wc),
             minid=0.95,
             maxindel=3,
             bwr=0.16,
             bw=12,
             trimq=10,
             qtrim="rl",
             flags="quickmatch fast untrim machineout",
             minhits=2,
             mem=icfg.mem("23g"),
             inout2="in2={input[1]} outu2={output.clean[1]} outm2={output.human[1]}"
    threads: 16
    conda:   "bbmap.yml"
    shell:
        "bbmap.sh "
        " minid={params.minid} "
        " maxindel={params.maxindel} "
        " bwr={params.bwr} "
        " bw={params.bw} "
        " {params.flags} "
        " minhits={params.minhits} "
        " path={input.reference} "
        " qtrim={params.qtrim} "
        " trimq={params.trimq} "
        " -Xmx{params.mem}m "
        " in={input[0]} "
        " outu={output.clean[0]} "
        " outm={output.human[0]} "
        " {params.inout2} "
        " threads={threads} "
        " {params.stats} "
        " > {log} 2>&1"


rule bbmap_split_se:  # ymp: extends bbmap_split
    input:          "{dir}/{sample}.{:pairnames[0]:}.fq.gz"
    output: clean = ["{dir}.bbmRM{reference}/{sample}.{:pairnames[0]:}.fq.gz"],
            human = ["{dir}.bbmKP{reference}/{sample}.{:pairnames[0]:}.fq.gz"]
    params: inout2 = ""


rule bmtagger_bitmask:
    message: "Bmtool indexing {input}"
    input:   "{path}.fasta"
    output:  "{path}.bitmask"
    log:     "{path}.bitmask.log"
    threads: 1
    params:  wordsize = 18,
             mem = icfg.mem("16g")
    conda:   "bmtagger.yml"
    shell: """
    bmtool \
        --fasta-file={input} \
        --output-file={output} \
        --word-size={params.wordsize} \
        > {log} 2>&1
    """
    # --compress fails with small references (segfault in bmfilter)


rule bmtagger_index:
    message: "Srcprism indexing {input}"
    input:   "{path}.fasta.gz"
    output:  touch("{path}.srprism")
    log:     "{path}.srprism.log"
    threads: 1
    params:  basename="{path}.srprism",
             mem = icfg.mem("8g")
    conda:   "bmtagger.yml"
    shell: """
    srprism mkindex \
        --input {input} \
        --output {output} \
        --memory {params.mem} \
        > {log} 2>&1
    """

ruleorder: gunzip > bmtagger_find
rule bmtagger_find:
    message: "Bmtagger find {wildcards.reference} {wildcards.dir}/{wildcards.sample}"
    input:   "{dir}/{sample}.{:pairnames:}.fq",
             bitmask = "{:dir.references:}/{reference}.bitmask",
             srprism = "{:dir.references:}/{reference}.srprism",
             tmpdir = ancient(icfg.dir.tmp)
    output:  temp("{dir}.bmtaggerRM{reference}/{sample}.txt"),
             "{dir}.bmtaggerRM{reference}/{sample}.txt.gz"
    log:     "{dir}.bmtaggerRM{reference}/{sample}.bmtagger.log"
    threads: 1
    params:  mem = icfg.mem("8g")
    conda:   "bmtagger.yml"
    shell: """
    bmtagger.sh \
        -b {input.bitmask} \
        -x {input.srprism} \
        -q 1 \
        -1 {input[0]} \
        -2 {input[1]} \
        -T {input.tmpdir} \
        -o {output[0]} \
        > {log} 2>&1
    gzip {output[0]} -c > {output[0]}.gz
    """


rule bmtagger_filter:
    message: "Bmtagger removing {wildcards.reference} {wildcards.dir}/{wildcards.sample}"
    wildcard_constraints:
        filt = "(RM|KP)"
    input:   "{dir}/{sample}.{pairsuff}.fq",
             idlist = "{dir}.bmtaggerRM{reference}/{sample}.txt"
    output:  "{dir}.bmtagger{filt}{reference}/{sample}.{pairsuff}.fq.gz"
    log:     "{dir}.bmtagger{filt}{reference}/{sample}.{pairsuff}.extract.log"
    threads: 8
    params:  filt = lambda wc: "-remove" if wc.filt == "RM" else "-keep",
             mem = icfg.mem("8g")
    conda:   "bmtagger.yml"
    shell: """
    extract_fullseq \
        {input.idlist} \
        {params.filt} \
        -fastq \
        -mate1 {input[0]} | pigz -p {threads} -9 > {output}  2>{log}
    """

###
### Primer Filtering
###

rule bbduk_primer:
    """
    Splits reads based on primer matching into "primermatch" and "primerfail".
    """
    message: "BBduk: Filtering {wildcards.sample} for primer set {input.primer}"
    input:   "{dir}/{sample}.{:pairnames:}.fq.gz",
             primer = "primers.fasta"
    output:  match  = "{dir}.primermatch/{sample}.{:pairnames:}.fq.gz",
             fail   = "{dir}.primerfail/{sample}.{:pairnames:}.fq.gz",
             stats  = expand("{{dir}}.primermatch/{{sample}}.{x}", x=bbduk_stats)
    log:     "{dir}.primermatch/{sample}.log"
    threads: 8
    params:
        stats=lambda wc: expand("{x}={dir}.primermatch/{sample}.{x}",x=bbduk_stats,**wc),
        mem=icfg.mem("80g"),
        k=12,
        rl=12,
        inout2="in2={input[1]} outm2={output.match[1]} outu2={output.fail[1]}"
    conda: "bbmap.yml"
    shell:
        'bbduk.sh'
        ' in={input[0]} outm={output.match[0]} outu={output.fail[0]} '
        ' {params.inout2} '
        ' ref={input.primer}'
        ' k={params.k}'               # match using k-mers
        ' restrictleft={params.rl} '  # only match leftmost n bases
        ' maskmiddle=f'               # don't mask middle base in kmer
        ' rcomp=f'                    # don't check reverse complement
        ' copyundefined=t'            # expand wobbles in input
        ' removeifeitherbad=f'        # "bad" is "match", we want both to match
        ' pigz unpigz'
        ' {params.stats}'
        ' -Xmx{params.mem}m'



rule bbduk_primer_se:  # ymp: extends bbduk_primer
    input:          "{dir}/{sample}.{:pairnames[0]:}.fq.gz"
    output: match = ["{dir}.primermatch/{sample}.{:pairnames[0]:}.fq.gz"],
            fail  = ["{dir}.primerfail/{sample}.{:pairnames[0]:}.fq.gz"]
    params: inout2 = ""
        
