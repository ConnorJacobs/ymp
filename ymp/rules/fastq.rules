from ymp.config import icfg
import os


"""
This file contains the rules dealing with FQ file provisioning and preparation. 
"""



###
###  SRA access 
###


localrules: prefetch
rule prefetch:
    """
    Downloads SRA files into NCBI SRA folder (ncbi/public/sra).
    """
    message:
        "Pre-Fetching {wildcards.SRR}"
    output:
        "{:sra:}/{SRR}.sra"
    conda:
        srcdir("sratools.yml")
    shell: """
    prefetch {wildcards.SRR}
    """


rule fastq_dump:
    """
    Extracts FQ from SRA files
    """
    message:
        "Extracting FastQ from {wildcards.SRR}"
#    input:
#        "{:sra:}/{SRR}.sra"
    output:
        "{:scratch:}/SRR/{SRR}_1.fastq.gz",
        "{:scratch:}/SRR/{SRR}_2.fastq.gz"
    params:
        outdir = icfg.scratchdir + "/SRR",
        p = lambda wc,threads: int(threads/2+.5)
    conda:
        srcdir("sratools.yml")
    threads:
        4
    # FIXME
    # the two cut processes use about 1 cpu each, fastqdump 1/4 and pgzip about 1 each.
    # not ideal. not sure why cut needs so much time. 
    shell: """
    fastq-dump {wildcards.SRR} \
        --split-files \
        --readids \
        --dumpbase \
        --skip-technical \
        --clip \
        --read-filter pass \
        --stdout | \
      paste - - - -  - - - - | \
      tee >(cut -f 1-4 | tr "\t" "\\n" | pigz -p {params.p} > {output[0]}) | \
      cut -f 5-8 | tr "\t" "\\n" | pigz -p {params.p} > {output[1]}
    """


###
###  Linking into current workspace
###
    

localrules: symlink_raw_reads
rule symlink_raw_reads:
    """Normalize FQ names by creating symlinks to original files"""
    message:
        "Creating symlink {output} -> {input}"
    input:
        # extract path from config file:
        lambda wc: icfg.FQpath(wc.project, wc.run, wc.pairsuff)
    output:
        "{project}/{run}.{pairsuff}.fq.gz"
    run:
        if not os.path.isabs(input[0]):
            input[0] = os.path.join("..", input[0])
        os.symlink(input[0], output[0])


localrules: symlink_reference_files
rule symlink_reference_files:
    message: "Preparing {input}"
    input:   lambda wc: icfg.ref[wc.ref],
             refdir = ancient(icfg.dir.references)
    output:  "{:dir.references:}/{ref}.fasta.gz"
    threads: 1
    run:
        if input[0].endswith(".gz"): # just symlink
            if not os.path.isabs(input[0]):
                input[0] = os.path.join("..", input[0])
            os.symlink(input[0], output[0])
        else: # compress
            shell("gzip -c {input[0]} > {output}")

###
### Meta rules
###

localrules: fq_all
rule fq_all:
    message: "Finished {wildcards.dir}"
    input: "{dir}/{:runs:}.{:pairnames:}.fq.gz"
    output: touch("{dir}/all")

rule fastqc_all:
    message: "Finished {wildcards.dir}"
    input: "{dir}.fastqc/{:runs:}.{:pairnames:}_fastqc.zip"
    output: touch("{dir}.fastqc/all")

###
###  Quality Reports
###


rule fastqc:
    """Run FastQC on read files"""
    message: "Creating QC report for {input}"
    input:   "{dir}/{file}.fq.gz"
    output:  "{dir}.fastqc/{file}_fastqc.html",
             "{dir}.fastqc/{file}_fastqc.zip"
    log:     "{dir}.fastqc/{file}_fastqc.log"
    threads: 1
    params:  k = 10
    conda:   srcdir("fastqc.yml")
    shell: """
    # override too low memory requested by fastqc 0.11.5 of 256M per thread:
    export _JAVA_OPTIONS="-Xmx$[{threads} * 8 * 1024]m"
    fastqc -t {threads} -o $(dirname {output[0]}) {input} -k 10 >{log} 2>&1
    """


rule multiqc:
    """Assemble report on all FQ files in a directory"""
    message: "Aggregating QC reports for {wildcards.dir}"
    input:   "{dir}.fastqc/{:runs:}.{:pairnames:}_fastqc.zip"
    output:  "{:reportsdir:}/{dir}.fastqc.html",
    log:     "{dir}.fastqc/multiqc.log"
    threads: 1
    params:  dir = "{dir}.fastqc"
    conda:   srcdir("multiqc.yml")
    shell: """
    cp `which multiqc` {params.dir}
    python -V > {params.dir}/python.log
    env > {params.dir}/env.log
    multiqc \
            --module fastqc \
            --outdir {params.dir} \
            --title  {wildcards.dir} \
            --force \
            {wildcards.dir}.fastqc \
            > {log} 2>&1
    if [ -e  {params.dir}/multiqc_report.html ]; then
        mv {params.dir}/multiqc_report.html {output[0]}
    else
        # never versions of multiqc put title in output file name
        mv {params.dir}/{wildcards.dir}_multiqc_report.html {output[0]}
    fi
    """


rule phyloFlash_makedb:
    """Have phyloFlash build its reference database"""
    message: "PhyloFlash running makedb.pl"
    output:  "{:dir.references:}/phyloFlash"
    input:   ssu="{:dir.references:}/ssu.fasta.gz",
             univec="{:dir.references:}/UniVec.fasta.gz"
    conda:   srcdir("phyloflash.yml")
    shell: """
    CWD="$PWD"
    mkdir -p {output}
    cd {output}
    ln -s "$CWD/{input.ssu}" SILVA_0_SSU.fasta.gz
    phyloFlash_makedb.pl \
      --silva_file SILVA_0_SSU.fasta.gz \
      --univec_file "$CWD/{input.univec}"
    """


rule phyloFlash:
    """Run PhyloFlash on samples"""
    message: "PhyloFlash {wildcards.dir}/{wildcards.sample}"
    input:   "{dir}/{sample}.{:pairnames:}.fq.gz",
             dbhome="{:dir.references:}/phyloFlash"
    output:  "{dir}.pf/{sample}.phyloFlash.NTUabundance.csv"
    log:     "{dir}.pf/{sample}.log"
    threads: 16
    conda:   srcdir("phyloflash.yml")
    shell: """
    cd {wildcards.dir}.pf

    phyloFlash.pl -skip_emirge \
                  -skip_spades \
                  -html \
                  -readlength 301 \
                  -read1 ../{input[0]} \
                  -read2 ../{input[1]} \
                  -lib {wildcards.sample} \
                  -CPUs {threads} \
                  -dbhome ../{input.dbhome}/0 \
                  > ../{log} 2>&1
    """


rule phyloFlash_heatmap:
    message: "PhyloFlash Heatmap {wildcards.dir}"
    input:   "{dir}.pf/{:runs:}.phyloFlash.NTUabundance.csv"
    output:  "{:reportsdir:}/{dir}_heatmap.pdf"
    log:     "{dir}.pf/phyloFlash_heatmap.log"
    threads: 1
    conda:   srcdir("phyloflash.yml")
    shell: """
    phyloFlash_heatmap.R {input[0]} {input} --out {output} >{log} 2>&1
    """


###
###  Error correction
###


rule bb_ecco:
    """Error correction with BBMerge overlapping"""
    message: "BBTools merge ecco {wildcards.dir}/{wildcards.sample}"
    input:   "{dir}/{sample}.{:pairnames:}.fq.gz"
    output:  "{dir}.ecco/{sample}.{:pairnames:}.fq.gz",
             adapter="{dir}.ecco/{sample}.adapter.fq"
    log:     "{dir}.ecco/{sample}.log"
    threads: 16
    conda:   srcdir("bbmap.yml")
    shell: """
    set -x
    java -version
    echo $PATH
    command -V java
    bbmerge.sh in={input[0]} in2={input[1]} out={output[0]} out2={output[1]} \
               outadapter={output.adapter} \
               ecco ecctadpole mix vstrict\
               threads={threads} -Xmx60g \
               > {log} 2>&1
    """


###
###  Trimming
###

rule trim_bbduk_adapter:
    """Trimming and Adapter Removal using BBTools BBDuk"""
    message:
        "Trimming with BBduk {wildcards.dir}/wildcards.sample "
        "({params.adapt}Q={params.qual} L={params.length})"
    input:
        "{dir}/{sample}.{:pairnames:}.fq.gz"
    wildcard_constraints:
        A = "(A?)",
        Q = "(Q\d+|)",
        L = "(L\d+|)"
    output:
        "{dir}.trim{A}{Q}{L}/{sample}.{:pairnames:}.fq.gz"
    log:
        "{dir}.trim{A}{Q}{L}/{sample}.log"
    params:
        length=lambda wc: wc.L[1:] if wc.L else 20,
        qual=lambda wc: wc.Q[1:] if wc.Q else 20,
        adapt=lambda wc: 'ref=$BB_RSRC/adapters.fa ' if wc.A else '',
        k=23,
        mink=11,
        hdist=1,
        mem=icfg.mem("80g"),
        flags="pigz unpigz"
    threads: 16
    conda: srcdir("bbmap.yml")
    shell:
        # find adapter dir:
        'BB_RSRC="$(dirname $(readlink -f $(command -v bbduk.sh)))/resources";'
        'bbduk.sh '
        'in={input[0]} in2={input[1]} '
        'out={output[0]} out2={output[1]} '
        'trimq={params.qual} qtrim=r '         # quality trimming
        'minlength={params.length} '           # length filtering
        '{params.adapt}'                       # adapter trimming
        'ktrim=r '                             # 3' side only
        'k={params.k} '                        # k for adapter matching
        'mink={params.mink} '                  # k at read end
        'hdist={params.hdist} '                # hamming distance, allow 1 mismatch
        'tpe ' # trimpairsevenly -- in case adapter detected in only one read
        'tbo ' # trimbyoverlap -- trim if read runs over other reads' end
        '{params.flags} '                      # processing settings
        'threads={threads} '
        '-Xmx{params.mem} '
        '>{log} 2>&1'



rule trimmomatic_adapter:
    """Trimming with Trimmomatic"""
    message:
        "Trimmomatic {wildcards.dir}/{wildcards.sample}"
    input:
        "{dir}/{sample}.{:pairnames:}.fq.gz"
    wildcard_constraints:
        adapter="(N|T(2|3|32))"
    output:
        p  = "{dir}.trimmomatic{adapter}/{sample}.{:pairnames:}.fq.gz",
        up = "{dir}.trimmomatic{adapter}/{sample}.unpaired.{:pairnames:}.fq.gz"
    log:
        "{dir}.trimmomatic{adapter}/{sample}.log"
    params:
        seed_mismatches = 2,
        palindrome_clip_thresh = 30,
        simple_clip_thresh = 10,
        min_adapter_len = 8,
        keep_both_reads = "true"
    conda:
        srcdir("trimmomatic.yml")
    threads:
        1
    shell:"""
    case {wildcards.adapter} in
      N)   ADAPTER=NexteraPE-PE.fa ;;
      T2)  ADAPTER=TruSeq2-PE.fa ;;
      T3)  ADAPTER=TruSeq3-PE.fa ;;
      T32) ADAPTER=TruSeq3-PE-2.fa ;;
    esac

    ADAPTER_DIR="$(dirname $(which trimmomatic))/../share/trimmomatic/adapters"

    CLIPARG="ILLUMINACLIP:$ADAPTER_DIR/$ADAPTER"
    CLIPARG="$CLIPARG:{params.seed_mismatches}"
    CLIPARG="$CLIPARG:{params.palindrome_clip_thresh}"
    CLIPARG="$CLIPARG:{params.simple_clip_thresh}"
    CLIPARG="$CLIPARG:{params.min_adapter_len}"
    CLIPARG="$CLIPARG:{params.keep_both_reads}"

    bash -x `which trimmomatic` PE \
        -threads {threads} \
        -phred33 \
        {input} \
        {output.p[0]} {output.up[0]} \
        {output.p[1]} {output.up[1]} \
        $CLIPARG >>{log} 2>&1
    """


rule sickle:
    message:
        "Trimming with Sickle {wildcards.dir}/{wildcards.sample} "
        "(Q={params.qual} L={params.length})"
    input:
        "{dir}/{sample}.{:pairnames:}.fq.gz"
    wildcard_constraints:
        Q = "(Q\d+|)",
        L = "(L\d+|)",
    output:
        "{dir}.sickle{Q}{L}/{sample}.{:pairnames:}.fq.gz",
        up = "{dir}.sickle{Q}{L}/{sample}.unpaired.fq.gz"
    log:
        "{dir}.sickle{Q}{L}/{sample}.log"
    params:
        length=lambda wc: wc.L[1:] if wc.L else 20,
        qual=lambda wc: wc.Q[1:] if wc.Q else 20
    threads: 1
    conda:
        srcdir("sickle.yml")
    shell:"""
    sickle pe \
        --pe-file1={input[0]} \
        --pe-file2={input[1]} \
        --qual-type=sanger \
        --output-pe1={output[0]} \
        --output-pe2={output[1]} \
        --output-single={output.up} \
        --length-threshold={params.length} \
        --qual-threshold={params.qual} \
        --gzip-output \
        --no-fiveprime \
        > {log} 2>&1
    """


###
###  Contaminant filtering
###


bbfiles="scafstats refstats "
bbfiles+="covstats rpkm covhist basecov bincov"

bbstats =  "bhist qhist aqhist bqhist lhist ihist ehist qahist "
bbstats += "indelhist mhist gchist idhist statsfile"
bbstats = bbstats.split()

bbduk_stats = "bhist qhist qchist aqhist bqhist lhist gchist".split()

rule bbmap_makedb:
    message: "BBMap: preparing index for ref={input}"
    input:   "{path}/{file}.fasta.gz"
    output:  "{path}/{file}.bbidx/ref/genome/1/summary.txt",
             "{path}/{file}.bbidx"
    log:     "{path}/{file}.bbidx/bbmap_index.log"
    params:  path="{path}/{file}.bbidx/"
    threads: 8
    conda:   srcdir("bbmap.yml")
    shell: """
    bbmap.sh \
        path={params.path} \
        ref={input} \
        threads={threads} \
        pigz unpigz \
        >{log} 2>&1
    """


rule bbmap_split:
    message: "BBMap filtering by {wildcards.reference} - {wildcards.dir}/{wildcards.sample}"
    input:   "{dir}/{sample}.{:pairnames:}.fq.gz",
             reference = "{:dir.references:}/{reference}.bbidx"
    output:  clean="{dir}.bbmRM{reference}/{sample}.{:pairnames:}.fq.gz",
             human="{dir}.bbmKP{reference}/{sample}.{:pairnames:}.fq.gz",
             stats=expand("{{dir}}.bbmRM{{reference}}/{{sample}}.{x}",x=bbstats)
    log:     "{dir}.bbmRM{reference}/{sapmple}.log"
    params:  stats=lambda wc: expand("{x}={dir}.bbmRM{reference}/{sample}.{x}",x=bbstats,**wc),
             minid=0.95,
             maxindel=3,
             bwr=0.16,
             bw=12,
             trimq=10,
             qtrim="rl",
             flags="quickmatch fast untrim machineout",
             minhits=2,
             mem=icfg.mem("23g")
    threads: 16
    conda:   srcdir("bbmap.yml")
    shell:
        "bbmap.sh "
        " minid={params.minid} "
        " maxindel={params.maxindel} "
        " bwr={params.bwr} "
        " bw={params.bw} "
        " {params.flags} "
        " minhits={params.minhits} "
        " path={input.reference} "
        " qtrim={params.qtrim} "
        " trimq={params.trimq} "
        " -Xmx{params.mem} "
        " in={input[0]} "
        " in2={input[1]} "
        " outu={output.clean[0]} "
        " outu2={output.clean[1]} "
        " outm={output.human[0]} "
        " outm2={output.human[1]} "
        " threads={threads} "
        " {params.stats} "
        " > {log} 2>&1"


rule bmtagger_bitmask:
    message: "Bmtool indexing {input}"
    input:   "{path}.fasta"
    output:  "{path}.bitmask"
    log:     "{path}.bitmask.log"
    threads: 1
    params:  wordsize = 18
    conda:   srcdir("bmtagger.yml")
    shell: """
    bmtool \
        --fasta-file={input} \
        --output-file={output} \
        --word-size={params.wordsize} \
        > {log} 2>&1
    """
    # --compress fails with small references (segfault in bmfilter)


rule bmtagger_index:
    message: "Srcprism indexing {input}"
    input:   "{path}.fasta.gz"
    output:  touch("{path}.srprism")
    log:     "{path}.srprism.log"
    threads: 1
    params:  basename="{path}.srprism",
             mem = 8
    conda:   srcdir("bmtagger.yml")
    shell: """
    srprism mkindex \
        --input {input} \
        --output {output} \
        --memory $(({params.mem} * 1024)) \
        > {log} 2>&1
    """

ruleorder: gunzip > bmtagger_find
rule bmtagger_find:
    message: "Bmtagger find {wildcards.reference} {wildcards.dir}/{wildcards.sample}"
    input:   "{dir}/{sample}.{:pairnames:}.fq",
             bitmask = "{:dir.references:}/{reference}.bitmask",
             srprism = "{:dir.references:}/{reference}.srprism",
             tmpdir = ancient(icfg.dir.tmp)
    output:  temp("{dir}.bmtaggerRM{reference}/{sample}.txt"),
             "{dir}.bmtaggerRM{reference}/{sample}.txt.gz"
    log:     "{dir}.bmtaggerRM{reference}/{sample}.bmtagger.log"
    threads: 1
    conda:   srcdir("bmtagger.yml")
    shell: """
    bmtagger.sh \
        -b {input.bitmask} \
        -x {input.srprism} \
        -q 1 \
        -1 {input[0]} \
        -2 {input[1]} \
        -T {input.tmpdir} \
        -o {output[0]} \
        > {log} 2>&1
    gzip {output[0]} -c > {output[0]}.gz
    """


rule bmtagger_filter:
    message: "Bmtagger removing {wildcards.reference} {wildcards.dir}/{wildcards.sample}"
    wildcard_constraints:
        filt = "(RM|KP)"
    input:   "{dir}/{sample}.{pairsuff}.fq",
             idlist = "{dir}.bmtaggerRM{reference}/{sample}.txt"
    output:  "{dir}.bmtagger{filt}{reference}/{sample}.{pairsuff}.fq.gz"
    log:     "{dir}.bmtagger{filt}{reference}/{sample}.{pairsuff}.extract.log"
    threads: 1
    params:  filt = lambda wc: "-remove" if wc.filt == "RM" else "-keep"
    conda:   srcdir("bmtagger.yml")
    shell: """
    extract_fullseq \
        {input.idlist} \
        {params.filt} \
        -fastq \
        -mate1 {input[0]} | pigz -p {threads} -9 > {output}  2>{log}
    """

###
### Primer Filtering
###

rule bbduk_primer:
    """
    Splits reads based on primer matching into "primermatch" and "primerfail".
    """
    message: "BBduk: Filtering {wildcards.sample} for primer set {input.primer}"
    input:   "{dir}/{sample}.{:pairnames:}.fq.gz",
             primer = "primers.fasta"
    output:  match  = "{dir}.primermatch/{sample}.{:pairnames:}.fq.gz",
             fail   = "{dir}.primerfail/{sample}.{:pairnames:}.fq.gz",
             stats  = expand("{{dir}}.primermatch/{{sample}}.{x}", x=bbduk_stats)
    log:     "{dir}.primermatch/{sample}.log"
    threads: 8
    params:
        stats=lambda wc: expand("{x}={dir}.primermatch/{sample}.{x}",x=bbduk_stats,**wc),
        mem=icfg.mem("80g"),
        k=12,
        rl=12
    conda: srcdir("bbmap.yml")
    shell:
        'bbduk.sh'
        ' in={input[0]} in2={input[1]}'
        ' outm={output.match[0]} outm2={output.match[1]}'
        ' outu={output.fail[0]} outu2={output.fail[1]}'
        ' ref={input.primer}'
        ' k={params.k}'               # match using k-mers
        ' restrictleft={params.rl} '  # only match leftmost n bases
        ' maskmiddle=f'               # don't mask middle base in kmer
        ' rcomp=f'                    # don't check reverse complement
        ' copyundefined=t'            # expand wobbles in input
        ' removeifeitherbad=f'        # "bad" is "match", we want both to match
        ' pigz unpigz'
        ' {params.stats}'
        ' -Xmx{params.mem}'


###
### De-duplication
###

        
rule bbmap_dedupe:
    message: "BBTools dedupe'ing {input}"
    input:   "{dir}/{sample}.{:pairnames:}.fq.gz"
    output:  "{dir}.ddp/{sample}.{:pairnames:}.fq.gz"
    log:     "{dir}.ddp/{sample}.log"
    params: mem=icfg.mem("80g")
    threads: 4
    conda:   srcdir("bbmap.yml")
    shell:
        "dedupe.sh"
        " unpigz"
        " threads={threads}"
        " in={input[0]}"
        " in2={input[1]}"
        " out=stdout"
        " -Xmx{params.mem} "
        " 2>{log}"
        " |"
        " paste - - - -  - - - - | "
        " tee >(cut -f 1-4 | tr \"\t\" \"\\n\" | pigz -p {threads} > {output[0]}) | "
        " cut -f 5-8 | tr \"\t\" \"\\n\" | "
        " pigz -p {threads} > {output[1]}"
        

