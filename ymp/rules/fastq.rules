from ymp.config import icfg
import os


"""
This file contains the rules dealing with FQ file provisioning and preparation. 
"""



###
###  SRA access 
###


localrules: prefetch
rule prefetch:
    """
    Downloads SRA files into NCBI SRA folder (ncbi/public/sra).
    """
    message:
        "Pre-Fetching {wildcards.SRR}"
    output:
        "{:sra:}/{SRR}.sra"
    conda:
        srcdir("sratools.yml")
    shell: """
    prefetch {wildcards.SRR}
    """


rule fastq_dump:
    """
    Extracts FQ from SRA files
    """
    message:
        "Extracting FastQ from {wildcards.SRR}"
#    input:
#        "{:sra:}/{SRR}.sra"
    output:
        "{:scratch:}/SRR/{SRR}_1.fastq.gz",
        "{:scratch:}/SRR/{SRR}_2.fastq.gz"
    params:
        outdir = icfg.scratchdir + "/SRR",
        p = lambda wc,threads: int(threads/2+.5)
    conda:
        srcdir("sratools.yml")
    threads:
        4
    # FIXME
    # the two cut processes use about 1 cpu each, fastqdump 1/4 and pgzip about 1 each.
    # not ideal. not sure why cut needs so much time. 
    shell: """
    fastq-dump {wildcards.SRR} \
        --split-files \
        --readids \
        --dumpbase \
        --skip-technical \
        --clip \
        --read-filter pass \
        --stdout | \
      paste - - - -  - - - - | \
      tee >(cut -f 1-4 | tr "\t" "\\n" | pigz -p {params.p} > {output[0]}) | \
      cut -f 5-8 | tr "\t" "\\n" | pigz -p {params.p} > {output[1]}
    """


###
###  Linking into current workspace
###
    

localrules: symlink_raw_reads
rule symlink_raw_reads:
    """Normalize FQ names by creating symlinks to original files"""
    message:
        "Creating symlink {output} -> {input}"
    input:
        # extract path from config file:
        lambda wc: icfg.FQpath(wc.project, wc.run, wc.pairsuff)
    output:
        "{project}/{run}.{pairsuff}.fq.gz"
    run:
        if not os.path.isabs(input[0]):
            input[0] = os.path.join("..", input[0])
        os.symlink(input[0], output[0])

rule symlink_all_raw_reads:
    message:
        "Linked all input FQ files"
    input:
        lambda wc: expand(
            "{project}/{run}.{pairsuff}.fq.gz",
            project=wc.project,
            run=icfg.getRuns(wc.project),
            pairsuff=icfg.pairnames
        )
    output:
        "{project}/all"
    shell: "touch {output}"


###
###  Quality Reports
###


rule fastqc:
    """Run FastQC on read files"""
    message: "Creating QC report for {input}"
    input:   "{dir}/{file}.fq.gz"
    output:  "{dir}_qc/{file}_fastqc.html",
             "{dir}_qc/{file}_fastqc.zip"
    log:     "{dir}_qc/{file}_fastqc.log"
    threads: 1
    params:  k = 10
    conda:   srcdir("fastqc.yml")
    shell: """
    # override too low memory requested by fastqc 0.11.5 of 256M per thread:
    export _JAVA_OPTIONS="-Xmx$[{threads} * 8 * 1024]m"
    fastqc -t {threads} -o {wildcards.dir}_qc {input} -k 10 >{log} 2>&1
    """


rule multiqc:
    """Assemble report on all FQ files in a directory"""
    message: "Aggregating QC reports for {wildcards.dir}"
    input:   "{dir}_qc/{:runs:}.{:pairnames:}_fastqc.html"
    output:  "{:reportsdir:}/{dir}_qc.html",
             "{dir}_qc/multiqc_data"
    log:     "{dir}_qc/multiqc.log"
    threads: 1
    conda:   srcdir("multiqc.yml")
    shell: """
    multiqc \
            --module fastqc \
            --outdir {wildcards.dir}_qc \
            --title  {wildcards.dir} \
            --force \
            {wildcards.dir}_qc \
            > {log} 2>&1
    mv {wildcards.dir}_qc/multiqc_report.html {output[0]}
    """


rule phyloFlash:
    """Run PhyloFlash on samples"""
    message: "PhyloFlash {wildcards.dir}/{wildcards.sample}"
    input:   "{dir}/{sample}.{:pairnames:}.fq.gz"
    output:  "{dir}.pf/{sample}.phyloFlash.NTUabundance.csv"
    log:     "{dir}.pf/{sample}.log"
    threads: 16
    conda:   srcdir("phyloflash.yml")
    shell: """
    module load phyloFlash
    cd {wildcards.dir}.pf

    phyloFlash.pl -skip_emirge \
                  -skip_spades \
                  -html \
                  -readlength 301 \
                  -read1 ../{input[0]} \
                  -read2 ../{input[1]} \
                  -lib {wildcards.sample} \
                  -CPUs {threads} \
                  > ../{log} 2>&1
    """


rule phyloFlash_heatmap:
    message: "PhyloFlash Heatmap {wildcards.dir}"
    input:   "{dir}.pf/{:runs:}.phyloFlash.NTUabundance.csv"
    output:  "{:reportsdir:}/{dir}_heatmap.pdf"
    log:     "{dir}.pf/phyloFlash_heatmap.log"
    threads: 1
    conda:   srcdir("phyloflash.yml")
    shell: """
    module load phyloFlash
    phyloFlash_heatmap.R -t '' {input} --out {output} >{log} 2>&1
    """


###
###  Error correction
###


rule bb_ecco:
    """Error correction with BBMerge overlapping"""
    message: "BBTools merge ecco {wildcards.dir}/{wildcards.sample}"
    input:   "{dir}/{sample}.{:pairnames:}.fq.gz"
    output:  "{dir}.ecco/{sample}.{:pairnames:}.fq.gz",
             adapter="{dir}.ecco/{sample}.adapter.fq"
    log:     "{dir}.ecco/{sample}.log"
    threads: 16
    conda:   srcdir("bbmap.yml")
    shell: """
    bbmerge.sh in={input[0]} in2={input[1]} out={output[0]} out2={output[1]} \
               outadapter={output.adapter} \
               ecco ecctadpole mix vstrict\
               threads={threads} -Xmx60g \
               > {log} 2>&1
    """


###
###  Trimming
###

# FIXME: this doesn't work with bbmap from conda!
rule trim_bbduk_adapter:
    """Trimming and Adapter Removal using BBTools BBDuk"""
    message:  "BBDuk A+Q trimming at Q{wildcards.Q}:"
              "{wildcards.dir}/{wildcards.sample}"
    input:    "{dir}/{sample}.{:pairnames:}.fq.gz"
    wildcard_constraints:  Q = "\d+"
    output:   "{dir}.trimAQ{Q}/{sample}.{:pairnames:}.fq.gz"
    threads:  16
    conda:    srcdir("bbmap.yml")
    shell: """
    ADAPTER_DIR="$(dirname $(readlink -f $(command -v bbduk.sh)))/resources"

    bbduk.sh in={input[0]} in2={input[1]} out={output[0]} out2={output[1]} \
             trimq={wildcards.Q} qtrim=r \
             ref=$ADAPTER_DIR/adapters.fa ktrim=r k=23 mink=11 hdist=1 tpe tbo \
             pigz unpigz threads={threads} -Xmx80g
    """


rule trimmomatic_adapter:
    """
    """
    message:
        "Trimmomatic {wildcards.dir}/{wildcards.sample}"
    input:
        "{dir}/{sample}.{:pairnames:}.fq.gz"
    wildcard_constraints:
        adapter="(N|T(2|3|32))"
    output:
        p  = "{dir}.trimmomatic{adapter}/{sample}.{:pairnames:}.fq.gz",
        up = "{dir}.trimmomatic{adapter}/{sample}.unpaired.{:pairnames:}.fq.gz"
    log:
        "{dir}.trimmomatic{adapter}/{sample}.log"
    params:
        seed_mismatches = 2,
        palindrome_clip_thresh = 30,
        simple_clip_thresh = 10,
        min_adapter_len = 8,
        keep_both_reads = "true"
    conda:
        srcdir("trimmomatic.yml")
    threads:
        1
    shell:"""
    case {wildcards.adapter} in
      N)   ADAPTER=NexteraPE-PE.fa ;;
      T2)  ADAPTER=TruSeq2-PE.fa ;;
      T3)  ADAPTER=TruSeq3-PE.fa ;;
      T32) ADAPTER=TruSeq3-PE-2.fa ;;
    esac

    ADAPTER_DIR="$(dirname $(which trimmomatic))/../share/trimmomatic/adapters"

    CLIPARG="ILLUMINACLIP:$ADAPTER_DIR/$ADAPTER"
    CLIPARG="$CLIPARG:{params.seed_mismatches}"
    CLIPARG="$CLIPARG:{params.palindrome_clip_thresh}"
    CLIPARG="$CLIPARG:{params.simple_clip_thresh}"
    CLIPARG="$CLIPARG:{params.min_adapter_len}"
    CLIPARG="$CLIPARG:{params.keep_both_reads}"

    trimmomatic PE \
        -threads {threads} \
        -phred33 \
        {input} \
        {output.p[0]} {output.up[0]} \
        {output.p[1]} {output.up[1]} \
        $CLIPARG >{log} 2>&1
    """


rule sickle:
    message:
        "Trimming with Sickle {wildcards.dir}/{wildcards.sample} "
        "(Q={params.qual} L={params.length})"
    input:
        "{dir}/{sample}.{:pairnames:}.fq.gz"
    wildcard_constraints:
        Q = "(Q\d+|)",
        L = "(L\d+|)",
    output:
        "{dir}.sickle{Q}{L}/{sample}.{:pairnames:}.fq.gz",
        up = "{dir}.sickle{Q}{L}/{sample}.unpaired.fq.gz"
    log:
        "{dir}.sickle{Q}{L}/{sample}.log"
    params:
        length=lambda wc: wc.L[1:] if wc.L else 20,
        qual=lambda wc: wc.Q[1:] if wc.Q else 20
    threads: 1
    shell:"""
    sickle pe \
        --pe-file1={input[0]} \
        --pe-file2={input[1]} \
        --qual-type=sanger \
        --output-pe1={output[0]} \
        --output-pe2={output[1]} \
        --output-single={output.up} \
        --length-threshold={params.length} \
        --qual-threshold={params.qual} \
        --gzip-output \
        --no-fiveprime \
        > {log} 2>&1
    """


###
###  Contaminant filtering
###


bbfiles="scafstats refstats "
bbfiles+="covstats rpkm covhist basecov bincov"

bbstats =  "bhist qhist aqhist bqhist lhist ihist ehist qahist "
bbstats += "indelhist mhist gchist idhist statsfile"
bbstats = bbstats.split()

rule bbmap_makedb:
    message: "BBMap: preparing index for ref={input}"
    input:   "{path}/{file}.{fagz}"
    output:  "{path}/{file}.{fagz}.bbidx/ref/genome/1/summary.txt",
             "{path}/{file}.{fagz}.bbidx"
    log:     "{path}/{file}.{fagz}.bbidx/bbmap_index.log"
    params:  path="{path}/{file}.bbidx/",
             fagz="{fagz}"
    threads: 8
    conda:   srcdir("bbmap.yml")
    shell: """
    bbmap.sh \
        path={params.path} \
        ref={input} \
        threads={threads} \
        pigz unpigz \
        >{log} 2>&1
    """


rule bbmap_rmhuman:
    message: "BBTools removing human reads from {input}"
    input:   "{dir}/{sample}.{:pairnames:}.fq.gz"
    output:  clean="{dir}.xhum/{sample}.{:pairnames:}.fq.gz",
             human="{dir}.xhum/{sample}.human.{:pairnames:}.fq.gz",
             stats=expand("{{dir}}.xhum/{{sample}}.{x}",x=bbstats)
    log:     "{dir}.xhum/{sample}.log"
    params:  stats=lambda wc: expand("{x}={dir}.xhum/{sample}.{x}",x=bbstats,**wc),
             minid=0.95,
             maxindel=3,
             bwr=0.16,
             bw=12,
             trimq=10,
             qtrim="rl",
             flags="quickmatch fast untrim machineout",
             minhits=2,
             mem=23,
             path=icfg.db.human
    threads: 16
    conda:   srcdir("bbmap.yml")
    shell:
        "bbmap.sh "
        " minid={params.minid} "
        " maxindel={params.maxindel} "
        " bwr={params.bwr} "
        " bw={params.bw} "
        " {params.flags} "
        " minhits={params.minhits} "
        " path={params.path} "
        " qtrim={params.qtrim} "
        " trimq={params.trimq} "
        " -Xmx{params.mem}g "
        " in={input[0]} "
        " in2={input[1]} "
        " outu={output.clean[0]} "
        " outu2={output.clean[1]} "
        " outm={output.human[0]} "
        " outm2={output.human[1]} "
        " threads={threads} "
        " {params.stats} "
        " > {log} 2>&1"



from snakemake.remote.HTTP import RemoteProvider as HTTPRemoteProvider
HTTP = HTTPRemoteProvider()
localrules: bmtagger_get_hs37
rule bmtagger_get_hs37:
    message: "Downloading hs37"
    input:   HTTP.remote("https://ftp.ncbi.nlm.nih.gov/pub/agarwala/bmtagger/hs37.fa")
    output:  "references/hs37.fa"
    threads: 1
    shell:   "mv {input} {output}"


rule bmtagger_bitmask:
    message: "Bmtool indexing {input}"
    input:   "{path}.fa"
    output:  "{path}.bitmask"
    log:     "{path}.bitmask.log"
    threads: 1
    params:  wordsize = 18
    conda:   srcdir("bmtagger.yml")
    shell: """
    bmtool \
        --fasta-file={input} \
        --output-file={output} \
        --word-size={params.wordsize} \
        --compress \
        > {log} 2>&1
    """


rule bmtagger_index:
    message: "Srcprism indexing {input}"
    input:   "{path}.fa"
    output:  touch("{path}.srprism")
    log:     "{path}.srprism.log"
    threads: 1
    params:  basename="{path}.srprism",
             mem = 8
    conda:   srcdir("bmtagger.yml")
    shell: """
    srprism mkindex \
        --input {input} \
        --output {output} \
        --memory $(({params.mem} * 1024)) \
        > {log} 2>&1
    """


rule bmtagger_find:
    message: "Bmtagger find {wildcards.reference} {wildcards.dir}/{wildcards.sample}"
    input:   "{dir}/{sample}.{:pairnames:}.fq",
             bitmask = "references/{reference}.bitmask",
             srprism = "references/{reference}.srprism"
    output:  temp("{dir}.bmtagger{reference}/{sample}.txt"),
             "{dir}.bmtagger{reference}/{sample}.txt.gz"
    log:     "{dir}.bmtagger{reference}/{sample}.bmtagger.log"
    threads: 1
    params:  tmpdir = "/scratch/$HOME/tmp"
    conda:   srcdir("bmtagger.yml")
    shell: """
    bmtagger.sh \
        -b {input.bitmask} \
        -x {input.srprism} \
        -q 1 \
        -1 {input[0]} \
        -2 {input[1]} \
        -T {params.tmpdir} \
        -o {output[0]} \
        > {log} 2>&1
    gzip {output[0]} -c > {output[0]}.gz
    """


rule bmtagger_remove:
    message: "Bmtagger removing {wildcards.reference} {wildcards.dir}/{wildcards.sample}"
    input:   "{dir}/{sample}.{pairsuff}.fq",
             idlist = "{dir}.bmtagger{reference}/{sample}.txt"
    output:  "{dir}.bmtagger{reference}/{sample}.{pairsuff}.fq.gz"
    log:     "{dir}.bmtagger{reference}/{sample}.extract.log"
    threads: 1
    params:  tmpdir = "/scratch/$HOME/tmp"
    conda:   srcdir("bmtagger.yml")
    shell: """
    extract_fullseq \
        {input.idlist} \
        -remove \
        -fastq \
        -mate1 {input[0]} | pigz -p {threads} -9 > {output}  2>{log}

    """


###
### De-duplication
###

        
rule bbmap_dedupe:
    message: "BBTools dedupe'ing {input}"
    input:   "{dir}/{sample}.{:pairnames:}.fq.gz"
    output:  "{dir}.ddp/{sample}.{:pairnames:}.fq.gz"
    log:     "{dir}.ddp/{sample}.log"
    threads: 4
    conda:   srcdir("bbmap.yml")
    shell:
        "dedupe.sh"
        " unpigz"
        " threads={threads}"
        " in={input[0]}"
        " in2={input[1]}"
        " out=stdout"
        " 2>{log}"
        " |"
        " paste - - - -  - - - - | "
        " tee >(cut -f 1-4 | tr \"\t\" \"\n\" | pigz -p {threads} > {output[0]}) | "
        " cut -f 5-8 | tr \"\t\" \"\n\" | "
        " pigz -p {threads} > {output[1]}"
        

