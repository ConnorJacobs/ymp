from ymp.snakemake import config
from ymp.config import icfg
from snakemake.io import expand

from ymp.util import R, glob_wildcards, read_propfiles, get_ncbi_root
from config import dir2targets, dir2targets2

import glob
import yaml

# make jobs always list the host on which they ran
shell.prefix("hostname;")

#RAW     = "00_raw_reads"
RAW = "raw"
TRIMMED = "10_trimmed_reads"


localrules: bbmap_index, all_fasta, all_fq


rule multiqc_bysample:
    message: "Aggregating QC reports for {wildcards.sample}: {input}"
    output: "{sample}_q.html"
    input: lambda wc: glob.glob("*/{}_fastqc.zip".format(wc.sample))
    shadow: "shallow"
    shell:"""
    set +x
    TMP="tmp_multiqc_bysample"
    mkdir $TMP
    cd $TMP
    for n in {input}; do
      folder=$(dirname $n)
      file=$(basename $n)
      unzip ../$n
      mv ${{file%.zip}} $folder
      sed -i 's/{wildcards.sample}/'$folder'/' $folder/fastqc_data.txt
    done
    cd ..
    multiqc --title {wildcards.sample} $TMP
    mv multiqc_report.html {output}
    """




rule split_by_length:
    message: "Splitting {input} by length"
    input: "{dir}/{sample}.{$PAIRS}.fq.gz"
    output: "{dir}.{length}plus/{sample}.{$PAIRS}.fq.gz",
            "{dir}.{length}minus/{sample}.{$PAIRS}.fq.gz"
    threads: 4
    shell:"""
    zcat {input[0]} | paste - - - - | awk '\
      {{\
        "zcat {input[1]} | paste - - - - " |& getline var;\
        split(var,v);\
        if (length($3) > {wildcards.length} || length(v[3]) > {wildcards.length}) {{\
          print var |& "sed s/\\\\\\\\t/\\\\\\\\n/g | gzip -c > {output[0]}"; \
          print $0  |& "sed s/\\\\\\\\t/\\\\\\\\n/g | gzip -c > {output[1]}" \
        }} else {{\
          print var |& "sed s/\\\\\\\\t/\\\\\\\\n/g | gzip -c > {output[2]}"; \
          print $0  |& "sed s/\\\\\\\\t/\\\\\\\\n/g | gzip -c > {output[3]}" \
        }}\
      }}\
    '
    for n in {output}; do
      if [ ! -e $n ]; then
        touch ${{n%.gz}}
        gzip ${{n%.gz}}
      fi
    done
    """

rule pool_by_column:
    message: "{rule}: Combining {input} into {output}"
    output: "{dir}.by_{column,[^.]+}/{colid}.{P}.fq.gz"
    input: lambda wc: expand("{dir}/{sample}.{P}.fq.gz",P=wc.P,dir=wc.dir,\
                             sample=[sample for sample in config['pe_sample'] \
                              if config['pe_sample'][sample][wc.column] == wc.colid])
    run:
        if len(input) == 1:
            os.symlink(os.path.relpath(input[0], os.path.dirname(output[0])),
                       output[0])
        elif len(input) == 0:
            print(wildcards.colid)
            print(wildcards.column)
            print(wildcards.P)
            print(wildcards.dir)
            wc=wildcards
            sample=[sample for sample in config['pe_sample'] \
                    if config['pe_sample'][sample][wc.column] == wc.colid]
            print("xx " + "::".join(sample))
            return False
        else:
            shell("zcat {input} | gzip > {output}")

    
rule all_pooled:
    message: "Completed all {wildcards.ext} in directory {wildcards.dir}"
    output: "{dir}/all_{ext}"
    input: lambda wc: expand("{dir}/{target}.{ext}", dir=wc.dir, ext=wc.ext, \
                             target=dir2targets(wc))
    shell: """
    touch {output}
    """

rule all_paired:
    output: "{dir}/all_fq.gz"
    input: "{dir}/all_{$PAIRS}.fq.gz"
    shell: "touch {output}"
    
                        




rule megahit_n50:
    message: "Gathering assembly statistics"
    input: "{dir}.mh/{$config[pe_samples]}.fasta"
    output: "{dir}.mh/n50.csv"
    run:
        input = [x for x in input if os.path.getsize(x) > 0]
        shell("""
        n50c.py {input} > {output}
        """)
    



rule spades_coassembly:
    message: "Co-Assembling {wildcards.dir} with spades"
    input: fwd = "{dir}/{$config[pe_samples]}.{$PAIRS[0]}.fq.gz",
           rev = "{dir}/{$config[pe_samples]}.{$PAIRS[1]}.fq.gz"
    output: "{dir}.spc/contigs.fasta"
    threads: 33
    run:
        non_empty = [os.path.getsize(r) > 0 and os.path.getsize(r) > 0
                     for f,r in zip(input.fwd, input.rev)]
        fwd = ["../" + x for x,k in zip(input.fwd, non_empty) if k]
        rev = ["../" + x for x,k in zip(input.rev, non_empty) if k]
        conf_yml = wildcards.dir + ".spc/conf.yml" 
        with open(conf_yml, "w") as f:
            f.write(yaml.dump([{
                "left reads": fwd,
                "right reads": rev,
                "type": "paired-end",
                "orientation": "fr"
            }]))
        shell("""
        module load spades
        spades.py -o {wildcards.dir}.spc -t {threads} -m 480 --dataset {conf_yml} \
                  --tmp-dir /scratch/$HOME/tmp --meta \
                  --continue
        """)
    

bbfiles="bincov.csv stats.txt".split()
rule bbmap_coassembly:
    output: "{dir}{by,.by_[^.]*}.{ass,[^./]+}.map/{sample}.bbmap.{$bbfiles}"
    input: fa = "{dir}.{ass}/contigs.fasta",
           fwd = "{dir}{by}/{sample}.{$PAIRS[0]}.fq.gz",
           rev = "{dir}{by}/{sample}.{$PAIRS[1]}.fq.gz"
    params: base = "{dir}{by}.{ass}.map/{sample}"
    threads: 17
    shell: """
    bbmap.sh jni threads={threads} machineout \
             nodisk ref={input.fa} \
             in={input.fwd} in2={input.rev} \
             statsfile={params.base}.bbmap.stats.txt \
             covstats={params.base}.bbmap.covstats.csv \
             covhist={params.base}.bbmap.covhist.csv \
             bincov={params.base}.bbmap.bincov.csv \

# These files are too large
#             basecov={params.base}.bbmap.basecov.csv.gz
    """

rule combine_bbstats:
    message: "Compiling stats table from bbmap runs"
    output: "{dir}/mapping_stats.csv"
    input: dir2targets2("{dir}/{sample}.bbmap.stats.txt")
    params: samples = lambda wc: dir2targets(wc)
    run:
        R("""
        f <- function(file, sample) {{
            read.csv(file, header=FALSE, sep="=", check.names=FALSE,
                     col.names=c("variable", sample))
        }}
        df <- Reduce(merge, mapply(f, input, params$samples, SIMPLIFY=FALSE))
        tdf <- data.frame(t(df[,-1]))
        colnames(tdf) <- df[,1]
        sample <- rownames(tdf)
        tdf <- sapply(tdf, function(x) as.numeric(sub("%", "", as.character(x))))
        tdf <- data.frame(sample, tdf)
        write.csv(tdf, file=unlist(output), row.names=FALSE)
        """, input=input, params=params, output=output)

        
rule combine_bincovs:
    message: "Merging coverage bins into table (in {wildcards.dir})"
    output: "{dir}/coverage.csv"
    input: dir2targets2("{dir}/{sample}.bbmap.bincov.csv")
    run:
        from merge import merge
        merge(output[0], input, collect=b"Cov")

rule filter_bincovs:
    message: "Filtering small bins from coverage"
    output: "{dir}/coverage_filtered.csv"
    input: "{dir}/coverage.csv"
    shell: """
    grep -Ev "[^,]*,(1000|[0-9]+[1-9][1-9][1-9])," {input} > {output}
    """

rule msg_canopy_cluster:
    message:
        "mgs-canopy {wildcards.dir}.cags"
    output:
        clust_t="{dir}.cags/clusters{mod}ssv",
        prof_t="{dir}.cags/profiles{mod}ssv",
        noobs="{dir}.cags/excl_insuf_obs{mod}ssv",
        domtop="{dir}.cags/excl_dom_top3{mod}ssv",
        stats="{dir}.cags/progress{mod}txt"
    input:
        "{dir}/coverage{mod}csv"
    log:
        "{dir}.cags/mgs-canopy{mod}log"
    threads: 33
    shell: """
    module load mgs-canopy
    mgs-canopy --input_file_path <(sed -n '2,$ {{y/ /_/;y/,/ /;s/ /_/;s/ /_/;p}}' {input}) \
               --output_clusters_file_path {output.clust_t} \
               --output_cluster_profiles_file {output.prof_t} \
               --cluster_name_prefix CAG \
               --num_threads {threads} \
               --verbosity info \
               --print_time_statistics > {log} \
               --die_on_kill \
               --not_processed_points_file {wildcards.dir}.cags/not_processed.txt \
               --filtered_out_points_min_obs_file {output.noobs} \
               --filtered_out_points_max_dominant_obs_file {output.domtop} \
               --canopy_size_stats_file {output.stats} 
    """

rule mgs_canopy_data_to_csv:
    message:
        "mgs-canopy {wildcards.dir}.cags (converting files)"
    input:
        clust_t = "{dir}.cags/clusters{mod}ssv",
        prof_t = "{dir}.cags/profiles{mod}ssv",
        head = "{dir}/coverage{mod}csv"
    output:
        clust = "{dir}.cags/clusters{mod}csv",
        prof = "{dir}.cags/profiles{mod}csv"
    shell: """
    (
        sed -n '1 s/\([^,]*,[^,]*,[^,]*\),.*/CAG,\\1/p' {input.head};
        sed 'y/\t/,/; s/\(.*\)_\([^_]*\)_\([^_]*\)/\\1,\\2,\\3/' {input.clust_t}
    ) > {output.clust}
    (
        sed -n '1 s/[^,]*,[^,]*,[^,]*/CAG/p' {input.head};
        sed 'y/\t/,/' {input.prof_t}
    ) > {output.prof}
    """


cags_complete, = glob_wildcards("{dir,[^/]*}.cags/log.txt")
rule summarize_cag_runs__grep:
    message: "Parsing CAG logfiles"
    output: temp("cag_stats.txt")
    input: expand("{cags_complete}.cags/log.txt", cags_complete=cags_complete)
    shell: """
    grep -P "^[^:]+: ?[0-9]+$" {input} > {output}
    """

rule summarize_cag_runs__R:
    message: "Tabularizing CAG stats"
    output: "cag_stats.csv"
    input: "cag_stats.txt"
    run:
        R("""
    library("reshape2")
    molten <- read.csv("{input}", header=FALSE, sep=":")
    colnames(molten) <- c("Run", "variable", "value")
    df <- dcast(molten, Run~variable)
    write.csv(df, "{output}", row.names=FALSE)
    """)
            

rule report_test:
    output:
        "reports/test.html"
    input:
        rmd=srcdir("CAGs.Rmd"),
        cags="raw.ecco.trimAQ20.290minus.by_SUBJECT.spc.map.cags/profiles_filtered.csv",
        ssu_otus="raw.ecco.trimAQ20.290minus.by_SUBJECT.SSU.emirge.otu97/otu.slv.cov100.csv"
    run:
        from ymp.util import Rmd
        Rmd(rmd=input.rmd[0],
            out=output,
            cags=os.path.abspath(input.cags[0]),
            ssu_otus=os.path.abspath(input.ssu_otus[0]))
        print(type(input.cags[0]))
